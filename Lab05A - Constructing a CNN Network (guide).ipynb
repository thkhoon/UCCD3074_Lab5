{"cells":[{"cell_type":"markdown","metadata":{"id":"2t42KE94qArc"},"source":["# Lab5A - Constructing a CNN Network\n","\n","For spatial data for example image or video data, Convolutional Neural Network (CNN or ConvNet) performs much better than  standard neural network. In this practical, we shall learn how to build a CNN Network.\n","\n","#### Objectives:\n","1. Learn how to build a convolutional neural network (CNN)\n","2. Learn how to build a network or layer using `sequential` "]},{"cell_type":"markdown","metadata":{"id":"XO8SoeLkukMh"},"source":["Remember to **enable the GPU** (Edit > Notebook setting > GPU) to ensure short training time."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":3514,"status":"ok","timestamp":1658656677860,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"J2kRNHKJqArg","outputId":"4812ef16-d91f-422a-fb29-06289d5ca250"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1658656677862,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"R26k7bWMqArm","outputId":"6e249a43-9583-4577-e3a1-65e444e66bbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/UCCD3074_Labs/UCCD3074_Lab5\n"]}],"source":["cd \"/content/gdrive/MyDrive/UCCD3074_Labs/UCCD3074_Lab5\""]},{"cell_type":"markdown","metadata":{"id":"FpacRGR5qArs"},"source":["Import the required libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Gtf8CwDdqArt","executionInfo":{"status":"ok","timestamp":1658656678406,"user_tz":-480,"elapsed":564,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","from torchsummary import summary\n","\n","from cifar10 import CIFAR10\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"gsxY9aORqAsO"},"source":["---\n","\n","# SECTION 1. DEFINING A CNN MODULE WITH `torch.nn.Module`\n","\n","In this section, we create a CNN network using `nn.Module`. The `Module` is the main building block, it defines the base class for all neural network and you MUST subclass it. \n","\n","## 1.1 Build the network\n","\n","**Exercise**. Build the following CNN. You will need to following modules:\n","* To define a conv2d layer: [`torch.nn.Conv2d(in_channel, out_channel, kernel_size, stride=1, padding=0)`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n","   * `in_channel`: number of channels in the input tensor. \n","   * `out_channel`: number of channels in the output tensor. This is equivalent to the number of filters in the current convolutional layer.\n","   * `kernel_size`: size of the filter (`f`).\n","   * `stride`: stride (`s`). Default value is 1.\n","   * `padding`: padding (`p`). Default value is 0.\n","\n","* To define a max pooling layer: [`torch.nn.functional.max_pool2d (x, kernel_size, stride=None, padding=0)`](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d)\n","   * `x`: input tensor of shape `(b, c, h, w)`. This is required as this is a `functional` operation.\n","   * `kernel_size`: size of the filter (`f`).\n","   * `stride`: stride (`s`). Default value is `kernel_size`.\n","   * `padding`: padding (`p`). Default value is 0.\n","\n","* To define a linear layer: [`torch.nn.Linear (in_features, out_features)`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n","  * `in_features`:  size of each input sample. This is equivalent to the number of units or signals in the previous layer.\n","  * `out_features`: size of the output sample. This is equivalent to the number of units / neurons in the current layer.\n","\n","* To define the global average pooling: [`torch.mean (x, dim)`](https://pytorch.org/docs/stable/generated/torch.mean.html)\n","    * `x`: the input tensor\n","    * `dim`: the dimensions to reduce. For the input tensor is `(b, c, h, w)`, to compute the mean of the spatial dimensions `h` and `w`, set `dim = [2, 3]`. This will compute the mean for the spatial dimensions and output a tensor of shape `(b, c, 1, 1)`. Then, use [`torch.squeeze`](https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze) to remove the two empty dimensions to get a tensor of shape `(b, c)`.\n","\n","* Alternatively, the global average pooling can be defined using the following command: [`torch.nn.AdaptiveAvgPool2d (output_size)`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)\n","    * `output size`: the target output size (`o`). The layer will configure the kernel size as `(input_size+target_size-1)//target_size` to generate an output tensor of shape `output_size`. \n","\n","<br><center><b>Network Architecture </b></center>\n","\n","|Layer | Name | Description | OutputShape |\n","|:--:|:--|:---:|---|\n","| - | Input       | -                            | (?,  3, 32, 32) |\n","| 1 | conv1       | Conv2d (k=32, f=3, s=1, p=1) | (?, 32, 32, 32) |\n","|   |             | relu                         | (?, 32, 32, 32) | \n","| 2 | conv2       | Conv2d (k=32, f=3, s=1, p=1) | (?, 32, 32, 32) | \n","|   |             | relu                         | (?, 32, 32, 32) |\n","|   | pool1       | maxpool (f=2, s =2, p=0)     | (?, 32, 16, 16) |\n","| 3 | conv3       | Conv2d (k=64, f=3, s=1, p=1) | (?, 64, 16, 16) | \n","|   |             | relu                         | (?, 64, 16, 16) |  \n","| 4 | conv4       | Conv2d (k=64, f=3, s=1, p=1) | (?, 64, 16, 16) | \n","|   |             | relu                         | (?, 64, 16, 16) |  \n","|   | global_pool | AdaptiveAvgPool (o=(1,1))    | (?, 64, 1, 1)   | \n","|   |             | view                         | (?, 64)         | \n","| 5 | fc1         | Linear (#units=10)           | (?, 10)         | \n","\n","Notes: `k`: number of filters, `f`: filter or kernel size, `s`: stride, `p`: padding, `o`: output shape\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"dWtkUN98qArz","executionInfo":{"status":"ok","timestamp":1658656678407,"user_tz":-480,"elapsed":10,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"uaxRAiLMqAsP","executionInfo":{"status":"ok","timestamp":1658656678410,"user_tz":-480,"elapsed":11,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["class Net1(nn.Module):\n","    def __init__(self):\n","        # call super constructor\n","        super().__init__()\n","\n","        # create the conv1 layer\n","        self.conv1 = nn.Conv2d(3,  32, kernel_size=3, stride=1, padding=1) \n","        \n","        # create the conv2 layer\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)  \n","        \n","        # create the conv3 layer\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)                  \n","        \n","        # create the conv4 layer\n","        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n","        \n","        # create the global pooling layer\n","        self.global_pool = nn.AdaptiveAvgPool2d((1,1)) \n","\n","        # fully connected layer\n","        self.fc1   = nn.Linear(64, 10) \n","        \n","    def forward(self, x):\n","\n","        # conv1 layer\n","        x = F.relu(self.conv1(x))\n","\n","        # conv2 layer\n","        x = F.relu(self.conv2(x))\n","        \n","        # pooling layer\n","        x = F.max_pool2d(x, 2, 2)\n","\n","        # conv3 layer\n","        x = F.relu(self.conv3(x))\n","        \n","        # conv4 layer\n","        x = F.relu(self.conv4(x))\n","\n","        # global pooling\n","        # x = torch.mean(x, [2, 3])\n","        x = self.global_pool(x)\n","\n","        # remove the spatial dimension\n","        x = x.squeeze()\n","\n","        # fc1 layer\n","        x = self.fc1(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"gMuWa40QjDmq"},"source":["Create the network and test it"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"-RUhB0J1kC-E","executionInfo":{"status":"ok","timestamp":1658656678411,"user_tz":-480,"elapsed":10,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["net1 = Net1()\n","out = net1(torch.randn(4, 3, 32, 32))"]},{"cell_type":"markdown","metadata":{"id":"UfbIVb5CUvkR"},"source":["Display the network"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658656678877,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"ZA6yNaOqvXiI","outputId":"1bffd3a0-a159-42c5-e6e0-53a18f7b598d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Net1(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc1): Linear(in_features=64, out_features=10, bias=True)\n",")\n"]}],"source":["print(net1)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658656678878,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"6BLNQcxaqAsg","outputId":"f510fb7d-34fe-4414-d47f-1bf04b536e0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 32, 32]             896\n","            Conv2d-2           [-1, 32, 32, 32]           9,248\n","            Conv2d-3           [-1, 64, 16, 16]          18,496\n","            Conv2d-4           [-1, 64, 16, 16]          36,928\n"," AdaptiveAvgPool2d-5             [-1, 64, 1, 1]               0\n","            Linear-6                   [-1, 10]             650\n","================================================================\n","Total params: 66,218\n","Trainable params: 66,218\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 0.75\n","Params size (MB): 0.25\n","Estimated Total Size (MB): 1.01\n","----------------------------------------------------------------\n"]}],"source":["summary(net1, input_size=(3, 32, 32), device=\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"9omfHlxWqAsj"},"source":["## 1.2 Load the dataset"]},{"cell_type":"markdown","metadata":{"id":"I4a9BnP1wtvi"},"source":["**Exercise**. Load the dataset\n","\n","1. Load the dataset. Define the following transformation pipeline to \n","   * Convert an image (numpy array with range (0, 255)) to a tensor, and \n","   *  Normalize the tensor with mean = 0.5 and std = 1.0 "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":3860,"status":"ok","timestamp":1658656682732,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"X0zbSP3sb681","outputId":"fe90af76-9965-4363-9393-67ccf38b1958"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Size of trainset: 10000\n","Size of testset: 2000\n"]}],"source":["# transform the model\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (1., 1., 1.))\n","])\n","\n","# Load the dataset\n","trainset = CIFAR10(train=True,  transform=transform, download=True, num_samples=10000)\n","testset  = CIFAR10(train=False,  transform=transform, download=True, num_samples=2000)\n","\n","print('Size of trainset:', len(trainset))\n","print('Size of testset:', len(testset))"]},{"cell_type":"markdown","metadata":{"id":"2L1gHLjfxXcS"},"source":["2. Create the dataloader for train set and test set. Use a batch size of 16, enable shuffle, apply the transformation pipeline defined above and use 2 cpu workers to load the datasets. "]},{"cell_type":"code","execution_count":10,"metadata":{"id":"8idPx0HxxTZB","executionInfo":{"status":"ok","timestamp":1658656682736,"user_tz":-480,"elapsed":16,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["trainloader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n","testloader  = DataLoader(testset, batch_size=16, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"3AyYjyqCyBiE"},"source":["## 1.3 Train the model\n","\n","**Exercise**. Complete the training function below"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"kkwr-hcnyNuj","executionInfo":{"status":"ok","timestamp":1658656682737,"user_tz":-480,"elapsed":13,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["def train(net, trainloader, num_epochs=15, lr=0.1, momentum=0.9):\n","    \n","    loss_iterations = int(np.ceil(len(trainloader)/3))\n","    \n","    # transfer model to GPU\n","    net = net.to(device)\n","    \n","    # set the optimizer. Use the SGD optimizer. Use the lr and momentum settings passed by the user\n","    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n","    \n","    # set to training mode\n","    net.train()\n","\n","    # variables\n","    best_loss = np.inf\n","    saturate_count = 0\n","    \n","    # train the network\n","    for e in range(num_epochs):    \n","\n","        running_loss = 0\n","        running_count = 0\n","\n","        # for all batch samples\n","        for i, (inputs, labels) in enumerate(trainloader):\n","\n","            # Clear all the gradient to zero\n","            optimizer.zero_grad()\n","\n","            # transfer data to GPU\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # forward propagation to get h\n","            outs = net(inputs)\n","\n","            # compute loss \n","            loss = F.cross_entropy(outs, labels)\n","\n","            # backpropagation to get gradients of all parameters\n","            loss.backward()\n","\n","            # update parameters\n","            optimizer.step()\n","\n","            # get the loss\n","            running_loss += loss.item()\n","            running_count += 1\n","\n","             # display the averaged loss value \n","            if i % loss_iterations == loss_iterations-1 or i == len(trainloader) - 1:                \n","                train_loss = running_loss / running_count\n","                running_loss = 0. \n","                running_count = 0.\n","                print(f'[Epoch {e+1:2d} Iter {i+1:5d}/{len(trainloader)}]: train_loss = {train_loss:.4f}')       \n","                \n","    print(\"Training completed.\")"]},{"cell_type":"markdown","metadata":{"id":"_Fh5B5cqypGQ"},"source":["Now, train the model with a maximum number of epochs of 50. The training will stop once it converge and may stop earlier. Use a learning rate of 0.01 and momentum of 0.9."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":69910,"status":"ok","timestamp":1658656752635,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"Hh5lypvVqAsk","outputId":"811d4e66-a41d-4850-e404-48d91e524251"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch  1 Iter   209/625]: train_loss = 2.3033\n","[Epoch  1 Iter   418/625]: train_loss = 2.2374\n","[Epoch  1 Iter   625/625]: train_loss = 2.1177\n","[Epoch  2 Iter   209/625]: train_loss = 2.0776\n","[Epoch  2 Iter   418/625]: train_loss = 2.0541\n","[Epoch  2 Iter   625/625]: train_loss = 1.9898\n","[Epoch  3 Iter   209/625]: train_loss = 1.9296\n","[Epoch  3 Iter   418/625]: train_loss = 1.8712\n","[Epoch  3 Iter   625/625]: train_loss = 1.8263\n","[Epoch  4 Iter   209/625]: train_loss = 1.8003\n","[Epoch  4 Iter   418/625]: train_loss = 1.7818\n","[Epoch  4 Iter   625/625]: train_loss = 1.7367\n","[Epoch  5 Iter   209/625]: train_loss = 1.7340\n","[Epoch  5 Iter   418/625]: train_loss = 1.7119\n","[Epoch  5 Iter   625/625]: train_loss = 1.6874\n","[Epoch  6 Iter   209/625]: train_loss = 1.6954\n","[Epoch  6 Iter   418/625]: train_loss = 1.6175\n","[Epoch  6 Iter   625/625]: train_loss = 1.6409\n","[Epoch  7 Iter   209/625]: train_loss = 1.6300\n","[Epoch  7 Iter   418/625]: train_loss = 1.5704\n","[Epoch  7 Iter   625/625]: train_loss = 1.5864\n","[Epoch  8 Iter   209/625]: train_loss = 1.5217\n","[Epoch  8 Iter   418/625]: train_loss = 1.5524\n","[Epoch  8 Iter   625/625]: train_loss = 1.5273\n","[Epoch  9 Iter   209/625]: train_loss = 1.5390\n","[Epoch  9 Iter   418/625]: train_loss = 1.4959\n","[Epoch  9 Iter   625/625]: train_loss = 1.4553\n","[Epoch 10 Iter   209/625]: train_loss = 1.4317\n","[Epoch 10 Iter   418/625]: train_loss = 1.4425\n","[Epoch 10 Iter   625/625]: train_loss = 1.3974\n","[Epoch 11 Iter   209/625]: train_loss = 1.3811\n","[Epoch 11 Iter   418/625]: train_loss = 1.3577\n","[Epoch 11 Iter   625/625]: train_loss = 1.3808\n","[Epoch 12 Iter   209/625]: train_loss = 1.3373\n","[Epoch 12 Iter   418/625]: train_loss = 1.3626\n","[Epoch 12 Iter   625/625]: train_loss = 1.3055\n","[Epoch 13 Iter   209/625]: train_loss = 1.3144\n","[Epoch 13 Iter   418/625]: train_loss = 1.2533\n","[Epoch 13 Iter   625/625]: train_loss = 1.2920\n","[Epoch 14 Iter   209/625]: train_loss = 1.2453\n","[Epoch 14 Iter   418/625]: train_loss = 1.2586\n","[Epoch 14 Iter   625/625]: train_loss = 1.2454\n","[Epoch 15 Iter   209/625]: train_loss = 1.2000\n","[Epoch 15 Iter   418/625]: train_loss = 1.1960\n","[Epoch 15 Iter   625/625]: train_loss = 1.2186\n","Training completed.\n"]}],"source":["train(net1, trainloader, num_epochs=15, lr=0.01, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"zJQVHE3tqAsr"},"source":["## 3. Evaluate the model\n","\n","Now let's evaluate the model. Remember that a 2-layered neural network only achieves an accuracy of around 38%. With a CNN architecture, you should be able to achieve a higher accuracy of more than 50%.\n","\n","**Exercise**. Complete the function `evaluate` below to evaluate the model on the test loader"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"mbuUhbily-Mw","executionInfo":{"status":"ok","timestamp":1658657072825,"user_tz":-480,"elapsed":414,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["def evaluate(net, testloader):\n","    \n","    # set to evaluation mode\n","    net.eval() \n","\n","    # running_correct\n","    running_corrects = 0\n","\n","    # Repeat for all batch data in the test set\n","    for inputs, targets in testloader:\n","\n","        # transfer to the GPU\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # # disable gradient computation\n","        with torch.no_grad():\n","            \n","            # perform inference\n","            outputs = net(inputs)\n","\n","            # predict as the best result  \n","            _, predicted = torch.max(outputs, 1)\n","\n","            running_corrects += (targets == predicted).double().sum()\n","\n","\n","    print('Accuracy = {:.2f}%'.format(100*running_corrects/len(testloader.dataset)))"]},{"cell_type":"markdown","metadata":{"id":"vVmtZB_fy-8C"},"source":["Now, let's evaluate our model."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":901,"status":"ok","timestamp":1658657074310,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"9Q0woXISqAst","outputId":"1a014d2b-7e53-4b63-f3d8-baef66d87f19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy = 52.65%\n"]}],"source":["evaluate(net1, testloader)"]},{"cell_type":"markdown","metadata":{"id":"Y-oxEglpqAsx"},"source":["---\n","# 2. CREATING A CNN NETWORK DIRECTLY USING torch.nn.Sequential\n","\n","In this section, we shall learn how to create a network using `torch.nn.Sequential`. `Sequential` is a container of `Modules` that can be stacked together and run at the same time. \n","\n","```\n","net = nn.Sequential(\n","    nn.Conv2d(....),\n","    nn.ReLU(),\n","    ....\n",")\n","\n","x = ... # get the input tensor\n","output = net(x)  # perform inference\n","```\n","\n","We can see immediately that it is a very convenient way to build a network.\n","\n","*Limitations: Note that you cannot add functional operations (e.g., `torch.relu`) into a `Sequential` model. If the `nn` module version does not exist for the function, then you have to create your own `nn` module for the function.*"]},{"cell_type":"markdown","metadata":{"id":"3hntw80gqAs4"},"source":["**Exercise**. Reimplement the network above using `torch.nn.Sequential`. \n","\n","Since you cannot use functional operations for `Sequential` models, you use their corresponding module versions:\n","* `torch.nn.functional.max_pool2d` --> [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n","* `torch.nn.functional.relu` --> [`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n","* `torch.view` --> [`nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"zjYESW_IqAs7","executionInfo":{"status":"ok","timestamp":1658657074313,"user_tz":-480,"elapsed":10,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["net2 = nn.Sequential(\n","    \n","    nn.Conv2d(3,  32, kernel_size=3, stride=1, padding=1),  # conv1\n","    nn.ReLU(), # relu\n","    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),  # conv2\n","    nn.ReLU(), # relu\n","    \n","    nn.MaxPool2d(kernel_size=2, stride=2, padding=0),       # max pool\n","\n","    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # conv3      \n","    nn.ReLU(),  # relu\n","    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # conv4\n","    nn.ReLU(), # relu\n","\n","    nn.AdaptiveAvgPool2d((1,1)), # global average pooling\n","    nn.Flatten(),   # flatten\n","    \n","    nn.Linear(64, 10) # fc1\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1658657074315,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"kDp67m2XJC_m","outputId":"2fb25b5f-1b9e-4a1a-e114-4cd9dcfb7f68"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 32, 32]             896\n","              ReLU-2           [-1, 32, 32, 32]               0\n","            Conv2d-3           [-1, 32, 32, 32]           9,248\n","              ReLU-4           [-1, 32, 32, 32]               0\n","         MaxPool2d-5           [-1, 32, 16, 16]               0\n","            Conv2d-6           [-1, 64, 16, 16]          18,496\n","              ReLU-7           [-1, 64, 16, 16]               0\n","            Conv2d-8           [-1, 64, 16, 16]          36,928\n","              ReLU-9           [-1, 64, 16, 16]               0\n","AdaptiveAvgPool2d-10             [-1, 64, 1, 1]               0\n","          Flatten-11                   [-1, 64]               0\n","           Linear-12                   [-1, 10]             650\n","================================================================\n","Total params: 66,218\n","Trainable params: 66,218\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.56\n","Params size (MB): 0.25\n","Estimated Total Size (MB): 1.83\n","----------------------------------------------------------------\n"]}],"source":["summary(net2, (3, 32, 32), device = \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"PB-IrMHsqAtC"},"source":["### Train the model"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":65494,"status":"ok","timestamp":1658657139800,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"Kl5_NdRPqAtD","outputId":"12ce1703-b1af-4bba-84f6-4a0878ef5084"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch  1 Iter   209/625]: train_loss = 2.3037\n","[Epoch  1 Iter   418/625]: train_loss = 2.2817\n","[Epoch  1 Iter   625/625]: train_loss = 2.1324\n","[Epoch  2 Iter   209/625]: train_loss = 2.1070\n","[Epoch  2 Iter   418/625]: train_loss = 2.0628\n","[Epoch  2 Iter   625/625]: train_loss = 2.0199\n","[Epoch  3 Iter   209/625]: train_loss = 1.9433\n","[Epoch  3 Iter   418/625]: train_loss = 1.8955\n","[Epoch  3 Iter   625/625]: train_loss = 1.8435\n","[Epoch  4 Iter   209/625]: train_loss = 1.8034\n","[Epoch  4 Iter   418/625]: train_loss = 1.7735\n","[Epoch  4 Iter   625/625]: train_loss = 1.7826\n","[Epoch  5 Iter   209/625]: train_loss = 1.7399\n","[Epoch  5 Iter   418/625]: train_loss = 1.7113\n","[Epoch  5 Iter   625/625]: train_loss = 1.7396\n","[Epoch  6 Iter   209/625]: train_loss = 1.6783\n","[Epoch  6 Iter   418/625]: train_loss = 1.6685\n","[Epoch  6 Iter   625/625]: train_loss = 1.6208\n","[Epoch  7 Iter   209/625]: train_loss = 1.6188\n","[Epoch  7 Iter   418/625]: train_loss = 1.6320\n","[Epoch  7 Iter   625/625]: train_loss = 1.5998\n","[Epoch  8 Iter   209/625]: train_loss = 1.5504\n","[Epoch  8 Iter   418/625]: train_loss = 1.5874\n","[Epoch  8 Iter   625/625]: train_loss = 1.5513\n","[Epoch  9 Iter   209/625]: train_loss = 1.5319\n","[Epoch  9 Iter   418/625]: train_loss = 1.4825\n","[Epoch  9 Iter   625/625]: train_loss = 1.5077\n","[Epoch 10 Iter   209/625]: train_loss = 1.4713\n","[Epoch 10 Iter   418/625]: train_loss = 1.4456\n","[Epoch 10 Iter   625/625]: train_loss = 1.4627\n","[Epoch 11 Iter   209/625]: train_loss = 1.4206\n","[Epoch 11 Iter   418/625]: train_loss = 1.4193\n","[Epoch 11 Iter   625/625]: train_loss = 1.3976\n","[Epoch 12 Iter   209/625]: train_loss = 1.3967\n","[Epoch 12 Iter   418/625]: train_loss = 1.3494\n","[Epoch 12 Iter   625/625]: train_loss = 1.3385\n","[Epoch 13 Iter   209/625]: train_loss = 1.3304\n","[Epoch 13 Iter   418/625]: train_loss = 1.3122\n","[Epoch 13 Iter   625/625]: train_loss = 1.3069\n","[Epoch 14 Iter   209/625]: train_loss = 1.2950\n","[Epoch 14 Iter   418/625]: train_loss = 1.2506\n","[Epoch 14 Iter   625/625]: train_loss = 1.2692\n","[Epoch 15 Iter   209/625]: train_loss = 1.2458\n","[Epoch 15 Iter   418/625]: train_loss = 1.2469\n","[Epoch 15 Iter   625/625]: train_loss = 1.2405\n","Training completed.\n"]}],"source":["train(net2, trainloader, num_epochs=15, lr=0.01, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"dlYw_V9SqAtH"},"source":["### Evaluate the model on the test set"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":588,"status":"ok","timestamp":1658657140356,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"mJWpnTqnqAtI","outputId":"ef59eda8-0d0c-4751-f21b-ddcd53b83fd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy = 55.20%\n"]}],"source":["evaluate(net2, testloader)"]},{"cell_type":"markdown","metadata":{"id":"KV7MzAewqAtO"},"source":["---\n","# 3. Using a function to create a customizable BLOCK module\n","\n","In the following, we group the convolutional layers in the network above into 2 blocks:\n","* `conv1` and `conv2` --> `block_1`\n","* `conv3` and `conv4` --> `block_2`.\n","\n","This is how the network looks:\n","\n","<center> <b> NETWORK (with block) </b> </center>\n","\n","| Block |Layer | Name | Description | OutputShape |\n","|:---:|:--:|:--|:---:|:---:|\n","|input|-|-|-|(?, 3, 32, 32)|\n","||||||\n","| block_1 <br> (blk_cin=3, blk_cout=32) | 1 <br><br> - <br><br> 2 <br><br> - | conv1 <br><br> ReLU <br><br> conv2 <br><br> ReLU| Conv2d (in_channels=3, out_channels=32,f=3,s=1,p=1)<br><br> relu <br><br> Conv2d (in_channels=32, out_channels=32,f=3,s=1,p=1)<br><br> relu| (?, 32, 32, 32) <br><br> (?, 32, 32, 32)<br><br> (?, 32, 32, 32)<br><br> (?, 32, 32, 32) | \n","||||||\n","| - | - | pool1 | maxpool (f=2,s=2,p=0) | (?, 32, 16, 16) | \n","||||||\n","| block_2 <br> (blk_cin=32, blk_cout=64) | 3 <br><br> - <br><br> 4 <br><br> - | conv1 <br><br> ReLU <br><br> conv2 <br><br> ReLU| Conv2d (in_channels=32, out_channels=64,f=3,s=1,p=1)<br><br> relu <br><br> Conv2d (in_channels=64, out_channels=64, f=3,s=1,p=1)<br><br> relu| (?, 64, 16, 16) <br><br> (?, 64, 16, 16)<br><br> (?, 64, 16, 16)<br><br> (?, 64, 16, 16) | \n","||||||\n","|  | - | global_pool | AdaptiveAvgPool, o=(1,1) | (?, 64, 1, 1) | \n","|  | - | -           | view                     | (?, 64) | \n","||||||\n","|  | 5 | fc1         | Linear(#units=10)        | (?, 10) | \n","|  | - | -           | view                     | (?, 10) | \n","\n","<br>\n","<center> Notes: <i>k</i>: number of filters, <i>f</i>: filter or kernel size, <i>s</i>: stride, <i>p</i>: padding, <i>o</i>: output shape </center>\n","\n","\n","Comparing `block_1` and `block_2`, we find that they have similar structure\n","\n","`conv (blk_cin, blk_cout)  --> relu --> conv (blk_cout, blk_cout) --> relu` \n","\n","where \n","\n","* `blk_cin=3` and `blk_cout=32` for `block1`\n","* `blk_cin=32` and `blk_cout=64` for `block2`\n","\n","This means that we can use the same function to create `block1` and `block2`. we shall do precisely this next."]},{"cell_type":"markdown","metadata":{"id":"qKyDmVPsrQ7M"},"source":["### The BUILD_BLOCK function\n","\n","Rather than declaring each layer individually, you create a function `build_block` to create a block of layers. The function receives `in_ch`, the number of channels in the input tensor and `out_ch`, the number of channels in the output tensor.  In the following, complete the `build_block` function by returning a `nn.Sequential` module that builds and returns the following block\n","\n","| BLOCK (blk_cin, blk_cout) |\n","|:--:|\n","| Conv2d (in_channels=blk_cin, output_channels=blk_cout, f=3, s=1,p=1) |\n","| ReLU () |\n","| Conv2d (in_channels=blk_cout, output_channels=blk_cout, f=3, s=1,p=1) |\n","| ReLU () |\n","\n"]},{"cell_type":"code","source":["def build_block(blk_cin, blk_cout):\n","    block = nn.Sequential(\n","        nn.Conv2d(blk_cin,  blk_cout, kernel_size=3, stride=1, padding=1),  \n","        nn.ReLU(), \n","        nn.Conv2d(blk_cout, blk_cout, kernel_size=3, stride=1, padding=1),  \n","        nn.ReLU(), \n","    )\n","    return block"],"metadata":{"id":"Kc87PcnxSQYQ","executionInfo":{"status":"ok","timestamp":1658657140357,"user_tz":-480,"elapsed":9,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["block1 = build_block(3, 32)\n","print(block1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"8QpAi4X4UGoe","executionInfo":{"status":"ok","timestamp":1658657140358,"user_tz":-480,"elapsed":9,"user":{"displayName":"Hk T","userId":"15940850110230233514"}},"outputId":"dfca7f26-2f63-41f4-8f06-4e28e8a6aa36"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): ReLU()\n","  (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): ReLU()\n",")\n"]}]},{"cell_type":"markdown","source":["### Constructing the network with BUILD_BLOCK\n","\n","Now, construct `NETWORK (with block)`. Use the `build_block` function to construct `block1` and `block2`"],"metadata":{"id":"X3jmLul3UJpR"}},{"cell_type":"code","execution_count":23,"metadata":{"id":"-XgXW69YqAtO","executionInfo":{"status":"ok","timestamp":1658657140359,"user_tz":-480,"elapsed":7,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"outputs":[],"source":["class Net3(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # define block 1\n","        self.conv_block1 = build_block(3, 32)\n","        \n","        # define block 2\n","        self.conv_block2 = build_block(32, 64)\n","\n","        # define global_pool\n","        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n","        \n","        # define fc1\n","        self.fc1 = nn.Linear(64, 10)\n","        \n","    def forward(self, x):\n","        # block 1\n","        x = self.conv_block1(x)\n","        \n","        # max pool\n","        x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0)\n","        \n","        # block 2\n","        x = self.conv_block2(x)\n","        \n","        # global pool\n","        x = self.global_pool(x)\n","\n","        # view\n","        x = x.view(x.size(0), -1) \n","        \n","        # fc1\n","        x = self.fc1(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":652,"status":"ok","timestamp":1658657141005,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"J96YtlP3qAtT","outputId":"27a38253-1167-48bb-bb1b-740e34096edc"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 32, 32]             896\n","              ReLU-2           [-1, 32, 32, 32]               0\n","            Conv2d-3           [-1, 32, 32, 32]           9,248\n","              ReLU-4           [-1, 32, 32, 32]               0\n","            Conv2d-5           [-1, 64, 16, 16]          18,496\n","              ReLU-6           [-1, 64, 16, 16]               0\n","            Conv2d-7           [-1, 64, 16, 16]          36,928\n","              ReLU-8           [-1, 64, 16, 16]               0\n"," AdaptiveAvgPool2d-9             [-1, 64, 1, 1]               0\n","           Linear-10                   [-1, 10]             650\n","================================================================\n","Total params: 66,218\n","Trainable params: 66,218\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.50\n","Params size (MB): 0.25\n","Estimated Total Size (MB): 1.76\n","----------------------------------------------------------------\n"]}],"source":["net3 = Net3()\n","summary(net3, (3, 32, 32), device=\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"aj1K9m3PqAtY"},"source":["### Train the model"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":65693,"status":"ok","timestamp":1658657206694,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"slHmpzWrqAtY","outputId":"c587279b-8bd6-4377-84cf-6569d698a396"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch  1 Iter   209/625]: train_loss = 2.3043\n","[Epoch  1 Iter   418/625]: train_loss = 2.2616\n","[Epoch  1 Iter   625/625]: train_loss = 2.1163\n","[Epoch  2 Iter   209/625]: train_loss = 2.0972\n","[Epoch  2 Iter   418/625]: train_loss = 2.0199\n","[Epoch  2 Iter   625/625]: train_loss = 1.9518\n","[Epoch  3 Iter   209/625]: train_loss = 1.9106\n","[Epoch  3 Iter   418/625]: train_loss = 1.8369\n","[Epoch  3 Iter   625/625]: train_loss = 1.8175\n","[Epoch  4 Iter   209/625]: train_loss = 1.8142\n","[Epoch  4 Iter   418/625]: train_loss = 1.7458\n","[Epoch  4 Iter   625/625]: train_loss = 1.7503\n","[Epoch  5 Iter   209/625]: train_loss = 1.7100\n","[Epoch  5 Iter   418/625]: train_loss = 1.6953\n","[Epoch  5 Iter   625/625]: train_loss = 1.6882\n","[Epoch  6 Iter   209/625]: train_loss = 1.6642\n","[Epoch  6 Iter   418/625]: train_loss = 1.6411\n","[Epoch  6 Iter   625/625]: train_loss = 1.6486\n","[Epoch  7 Iter   209/625]: train_loss = 1.6068\n","[Epoch  7 Iter   418/625]: train_loss = 1.6026\n","[Epoch  7 Iter   625/625]: train_loss = 1.5702\n","[Epoch  8 Iter   209/625]: train_loss = 1.5390\n","[Epoch  8 Iter   418/625]: train_loss = 1.5445\n","[Epoch  8 Iter   625/625]: train_loss = 1.5442\n","[Epoch  9 Iter   209/625]: train_loss = 1.4911\n","[Epoch  9 Iter   418/625]: train_loss = 1.5032\n","[Epoch  9 Iter   625/625]: train_loss = 1.4851\n","[Epoch 10 Iter   209/625]: train_loss = 1.4244\n","[Epoch 10 Iter   418/625]: train_loss = 1.4538\n","[Epoch 10 Iter   625/625]: train_loss = 1.4037\n","[Epoch 11 Iter   209/625]: train_loss = 1.3941\n","[Epoch 11 Iter   418/625]: train_loss = 1.3626\n","[Epoch 11 Iter   625/625]: train_loss = 1.3757\n","[Epoch 12 Iter   209/625]: train_loss = 1.3141\n","[Epoch 12 Iter   418/625]: train_loss = 1.3364\n","[Epoch 12 Iter   625/625]: train_loss = 1.3011\n","[Epoch 13 Iter   209/625]: train_loss = 1.2824\n","[Epoch 13 Iter   418/625]: train_loss = 1.2968\n","[Epoch 13 Iter   625/625]: train_loss = 1.2925\n","[Epoch 14 Iter   209/625]: train_loss = 1.2376\n","[Epoch 14 Iter   418/625]: train_loss = 1.1726\n","[Epoch 14 Iter   625/625]: train_loss = 1.2761\n","[Epoch 15 Iter   209/625]: train_loss = 1.1994\n","[Epoch 15 Iter   418/625]: train_loss = 1.2135\n","[Epoch 15 Iter   625/625]: train_loss = 1.1916\n","Training completed.\n"]}],"source":["train(net3, trainloader, num_epochs=15, lr=0.01, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"vJDnBZ2RqAte"},"source":["### Evaluate the model"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":611,"status":"ok","timestamp":1658657207298,"user":{"displayName":"Hk T","userId":"15940850110230233514"},"user_tz":-480},"id":"sEFIHBp7qAtf","outputId":"32590ff7-9cc4-4ce4-88dd-3243b058abdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy = 52.65%\n"]}],"source":["evaluate(net3, testloader)"]},{"cell_type":"markdown","source":["---\n","# 4. Create a customizable BLOCK module\n","\n","We can also build a block by constructing a module called `BLOCK` where we can specify the `blk_cin` and `blk_cin` when constructing the block.\n","\n","\n","### The BLOCK module\n","\n","Implement the BLOCK module.\n","\n","| BLOCK (blk_cin, blk_cout) |\n","|:--:|\n","| Conv2d (in_channels=blk_cin, output_channels=blk_cout, f=3, s=1,p=1) |\n","| ReLU () |\n","| Conv2d (in_channels=blk_cout, output_channels=blk_cout, f=3, s=1,p=1) |\n","| ReLU () |\n","\n"],"metadata":{"id":"ojd-O6jekHje"}},{"cell_type":"code","source":["class BLOCK(nn.Module):\n","    def __init__(self, blk_cin, blk_cout):\n","        \n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(blk_cin,  blk_cout, kernel_size=3, stride=1, padding=1)  \n","        self.conv2 = nn.Conv2d(blk_cout, blk_cout, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, x):\n","\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))        \n","\n","        return x"],"metadata":{"id":"e6eJgI-9kP-n","executionInfo":{"status":"ok","timestamp":1658657207303,"user_tz":-480,"elapsed":33,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["### Construcing network with the BLOCK module\n","\n","Now, let's build the network using the `BLOCK` module that we have just created."],"metadata":{"id":"MJpoZ4QykrsX"}},{"cell_type":"code","source":["class Net4(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # define block 1\n","        self.conv_block1 = BLOCK(3, 32)\n","        \n","        # define block 2\n","        self.conv_block2 = BLOCK(32, 64)\n","\n","        # define global_pool\n","        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n","        \n","        # define fc1\n","        self.fc1 = nn.Linear(64, 10)\n","        \n","    def forward(self, x):\n","        # block 1\n","        x = self.conv_block1(x)\n","        \n","        # max pool\n","        x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0)\n","        \n","        # block 2\n","        x = self.conv_block2(x)\n","        \n","        # global pool\n","        x = self.global_pool(x)\n","\n","        # view\n","        x = x.view(x.size(0), -1) \n","        \n","        # fc1\n","        x = self.fc1(x)\n","        \n","        return x"],"metadata":{"id":"8nX2Q6nvkquI","executionInfo":{"status":"ok","timestamp":1658657207304,"user_tz":-480,"elapsed":33,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["net4 = Net4()"],"metadata":{"id":"Qg2lquDhlMMK","executionInfo":{"status":"ok","timestamp":1658657207310,"user_tz":-480,"elapsed":32,"user":{"displayName":"Hk T","userId":"15940850110230233514"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["### Train the model"],"metadata":{"id":"gWv-lg_nlIlW"}},{"cell_type":"code","source":["train(net4, trainloader, num_epochs=15, lr=0.01, momentum=0.9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"dCUIaF15lIwP","executionInfo":{"status":"ok","timestamp":1658657273810,"user_tz":-480,"elapsed":66530,"user":{"displayName":"Hk T","userId":"15940850110230233514"}},"outputId":"bb813704-ac00-41d1-ab0e-a8b6706f4b49"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch  1 Iter   209/625]: train_loss = 2.3015\n","[Epoch  1 Iter   418/625]: train_loss = 2.2380\n","[Epoch  1 Iter   625/625]: train_loss = 2.1052\n","[Epoch  2 Iter   209/625]: train_loss = 2.0804\n","[Epoch  2 Iter   418/625]: train_loss = 2.0293\n","[Epoch  2 Iter   625/625]: train_loss = 1.9657\n","[Epoch  3 Iter   209/625]: train_loss = 1.9291\n","[Epoch  3 Iter   418/625]: train_loss = 1.8748\n","[Epoch  3 Iter   625/625]: train_loss = 1.8325\n","[Epoch  4 Iter   209/625]: train_loss = 1.7820\n","[Epoch  4 Iter   418/625]: train_loss = 1.7922\n","[Epoch  4 Iter   625/625]: train_loss = 1.7768\n","[Epoch  5 Iter   209/625]: train_loss = 1.7579\n","[Epoch  5 Iter   418/625]: train_loss = 1.7392\n","[Epoch  5 Iter   625/625]: train_loss = 1.7319\n","[Epoch  6 Iter   209/625]: train_loss = 1.6866\n","[Epoch  6 Iter   418/625]: train_loss = 1.6857\n","[Epoch  6 Iter   625/625]: train_loss = 1.6418\n","[Epoch  7 Iter   209/625]: train_loss = 1.6643\n","[Epoch  7 Iter   418/625]: train_loss = 1.6227\n","[Epoch  7 Iter   625/625]: train_loss = 1.6382\n","[Epoch  8 Iter   209/625]: train_loss = 1.6152\n","[Epoch  8 Iter   418/625]: train_loss = 1.5588\n","[Epoch  8 Iter   625/625]: train_loss = 1.5742\n","[Epoch  9 Iter   209/625]: train_loss = 1.5690\n","[Epoch  9 Iter   418/625]: train_loss = 1.5363\n","[Epoch  9 Iter   625/625]: train_loss = 1.5349\n","[Epoch 10 Iter   209/625]: train_loss = 1.4852\n","[Epoch 10 Iter   418/625]: train_loss = 1.4738\n","[Epoch 10 Iter   625/625]: train_loss = 1.4711\n","[Epoch 11 Iter   209/625]: train_loss = 1.4782\n","[Epoch 11 Iter   418/625]: train_loss = 1.4293\n","[Epoch 11 Iter   625/625]: train_loss = 1.3731\n","[Epoch 12 Iter   209/625]: train_loss = 1.4311\n","[Epoch 12 Iter   418/625]: train_loss = 1.4043\n","[Epoch 12 Iter   625/625]: train_loss = 1.3351\n","[Epoch 13 Iter   209/625]: train_loss = 1.3347\n","[Epoch 13 Iter   418/625]: train_loss = 1.3368\n","[Epoch 13 Iter   625/625]: train_loss = 1.2996\n","[Epoch 14 Iter   209/625]: train_loss = 1.2829\n","[Epoch 14 Iter   418/625]: train_loss = 1.2901\n","[Epoch 14 Iter   625/625]: train_loss = 1.2729\n","[Epoch 15 Iter   209/625]: train_loss = 1.2537\n","[Epoch 15 Iter   418/625]: train_loss = 1.2475\n","[Epoch 15 Iter   625/625]: train_loss = 1.2435\n","Training completed.\n"]}]},{"cell_type":"markdown","source":["### Evaluate the model"],"metadata":{"id":"mzKAxLmPlQLI"}},{"cell_type":"code","source":["evaluate(net4, testloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"-NlJ_UAglRQB","executionInfo":{"status":"ok","timestamp":1658657273842,"user_tz":-480,"elapsed":60,"user":{"displayName":"Hk T","userId":"15940850110230233514"}},"outputId":"e012e96f-6650-43ef-cff8-930e80de4389"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy = 55.00%\n"]}]},{"cell_type":"markdown","metadata":{"id":"Df2XKfZlSM8R"},"source":["<center>--- End of Practical ---</center>"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Lab05A - Constructing a CNN Network (ans).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}